Ideas

Structure of physical network
Each participant is a node in the network
they supply performance data from a trigger pad through a trigger concentrator,
which supplies input to a single input port on the matrix. Therefore each must be on a unique channel in order
to route by port/channel to other devices. Note value is not crucial, with the requirement that inputs on same
channel across different concentrators must have a different note number. So, let's start with all inputs on
a single concentrator w/ same note number. The note number only is relevant when reaching the output device,
as it can control which timbre is heard. [Are there limits to the # of triggers/time on a single channel of output device?]
Output devices will to start have their own port from the matrix. Later, we can simply have a midi distribution amp from
a single output port. Keeping that in mind, each output should get output on a unique channel even now as they are specified
by matrix port...

so--a block of nodes devined by a single trigger -> MIDI concentrator

node	chan	note	matrixin		matrixout	chan
A		Ch1		n1			ip1				op1		Ch1
B		Ch2		n1			ip1				op2		Ch2
C		Ch3		n1			ip1				op3		Ch3
D
E
F
(G)

routing by port/channel in to port(=channel)out

E.g.

A -> B		ip1/Ch1 -> op2/Ch2
A -> C		ip1/Ch1 -> op3/Ch3
B -> A		ip1/Ch2 -> op1/Ch1
B -> C		ip1/Ch2 -> op3/Ch3
C -> A		ip1/Ch3 -> op1/Ch1

self feedback
x -> x		ip1/Chx -> opx/Chx

x -> y		ip1/Chx -> opy/Chy

will simplify addressing to think of nodes as #s: node# = inchan# = outport/outchan#
note number addresses the input concentrator (this, so bigbrother can distinguich each node's performance by combo of chan & note)

Now, each output device can respond to at most 6? different notes, so at first, each player could have a unique sound, but later,
with a larger network, multiple players would have to share the same sound, and possibly a single player might sound different
to different listeners. One solution is to make all sound the same! However that might be difficult? Key question--do we need
to be able to stream different performers, or merely be exposed to a field of identical sounds, with the network controling each
listener's 'view' of the whole.
Note: CAN vary velocity on a per-performer basis, so note/velocity could be unique to each person. But velocity adds an additional
issue of perceptual weighting of input. NOTE--in general , we can have a weighted network.!

Exp ideas: have two disconnected subnets entrain at different rates, then link the two subnets (through thin or thick connections)
and observe dynamics of this collision.

Topologies--either create as matrices/link files in matlab and load in to rhythmNetwork, or generate within network. Question of how
'dumb' to make RN program--merely a mediator between topology and the physical structure (as well as recording all activity in net)
or actually part of creating experiments. Certainly need to have control of induction sequences--these can be broadcast from
bigBrother, to some or all nodes.
In interest of progress--start as simple mediator, and generate nets elsewhere. Can add self-generation later.
#need to specify a net file structure, then. (x,y) pairs seem adequate, possibly with a weight value (x,y)w
node 0 can be bigBrother, could have alias for 'broadcast', e.g. (0,-1) meaning 0 -> all, but that's a frill
Now, timing: can assume first that all are connected at once, later can add pauses
Also need a preamble regarding induction? Does RN generate metronome & record activity, or is it simpler to have a general purpose
MIDI sequencer do this role?

Another need is sysex librarian for programming all the output drummachines en masse. Again, this could be built into the master program,
but to start would make sense to use an existing librarian program.
So: 
Matlab:			generate network specification files / analyze performance output
				keep track of participant data, human to node mapping.
				
RhythmNetwork:	translate net specification into physical connections in the MIOC system (could even be headless!)
				realitime display of network/performance data?
				
Sequencer:		generate induction and record performance output
				require: record MIDI channel info (e.g. into diff tracks)
				
sysexLib:		store, distribute drummachine programs
				[idea: drum machines could have induction sequUIences in them, triggered/clocked by sequencer]

One idea would be to have matlab be the point of user contact, w/ GUI and analysis, exp setup
it would then control and coordinate all other programs, ideally on commandline via system calls




topologies: nearest neighbor, ring, random, smallworld, start at once or build additively, link failures, different goals: synchrony,
maximal asynchrony (arbitration)

##additional design issues (12/28/04)
How to store the node addressing parameters? As above, to start, it's determined by node number, but later might want to add some
configurability. The info is stored in NSTapper objects.
1) array of NSTappers, using array controller to make a table view. Type into this to set things up. Need some way to save.
2) hard-wired (ok for now)
3) text file used to configure--some simple format that is read in and initializes the RNTappers.

I feel like RNTapper should be called RNNode--take care of the physical details, but don't worry about the name and address, or other
features of the tapper in this program. That can be recorded elsewhere. Make this one solely to set up the physical aspects of the
network.

So what needs to be done: 
start it up
initialize tapper nodes
use connection list to program MIOC
(possibly) plot graph of network
(possibly) show real time responses, node histograms, node stats, etc...
(possibly) start induction and record all activity
Change network on the fly (either at pre-programmed time, or using UI)
(possibly) change tapper node values and save [not needed]

The tension here is between doing it the stdc way and the cocoa way. Cocoa way is what I'd like to learn, but it's also not
natural right now (e.g. how to load a file, scan strings, etc), so I am hampered somewhat. Let's get something working, can always
change it later.

For now, removed the tapper node parameter editing--that's not likely to change, or need to change in an experiment. Make a
simple text file format for this. Remove names from tapper node object

###update (1/21/04)
Have come a long way! I've done it the 'cocoa' way, and the things I didn't know how to do then, I now understand and it's no problem.
Nice thing to be comfortable with now.

of the "what needs to be done" list above,
only thing remaining: start induction and record all activity, change network on the fly need still to be done.
Hardwired tapper node addresses are fine.

One change--have decided to use a one->many midi thru box on outputs, so each group of 6 tappers goes in and out of a single MIOC port
	This, rather than having each output have its own port. Reason being that this way it'll be easy to expand the network, and input
	and output are parallel.
	
So: now output and input ports are same
Just had an idea--can bigBrother's output be broken into multiple channels, so we can have e.g. several different metronomes
going to different subgroups of the network. This would be great for the split network, each part entrained to a different tempo
or phase, and then joining the networks. 
This could be specified as {0.1, 1}1.0 -- viz. BB channel 1 to tapper node 1, where BBchannel is from 1 to 16

Next is the structure of an experiment--this kind of thing would need ability to switch networks on the fly at different points of
the experiment--suddenly I'll have to write a language like in presentation to control it all
To start, could control manually?
What are the components?:  
1) induction sequence, defined by channel, IOI, length of sequence, start time
2) Network structure
3) (possibly) network delta (add/remove)--this might be more efficient than reprogramming the whole network each time? I'll have to see
	how long that actually takes
Each one of these things would have a start time
An experiment would simply be a dictionary (BTW can init NSDictionary from oldstyle plist files, and newer XML!)

General info-
nodes = #nodes
date = 
description =
notes =

Then an array of additional dictionaries, in sequence, for the different component events
Each would have a type
-----------------
type = stimulus
time = start time (in seconds from experiment start)
stimuli = Array of strings: channel (note), IOI, duration (e.g. 1(64):IOI=800.0, events=10, phase=0.0)--new class: RNStimulus
- initFromString
- start

-----------------
type = network, networkAdd, networkSubtract (different behaviors: replace vs delta)
time = start time
connections = array of strings defining connections

--ok, this is a lot, so take in stages
for now: generate induction sequence
record all midi

-----------------
1/24/05
induction sequence class is done--reads a simple definition string @ can generate a MIDIPacketList ready
	to send to MIDI device
Now working on experiment class to tie it all together

1/26/05
Experiment class nearly done, only recording of events needs to be done
Also started on multi-part experiment infrastructure
Wish list: it would be nice to interactively design networks...plus some algorithmically designed ones
Right now, counting on matlab to do that job

2/19/04
Been a while--diverted to sleep project and makeig talk. 
Experiment class now records all midi input, timestamping relative to experiment start. 
Have added start/stop/save methods, too. We create a dictionary and save it in xml plist format.
Whipped up a matlab routine to parse this format and create a structure! So, what's left?
(another aside--had to communicate w/ Thomas Elger, the miditemp engineer, to find out the correct checksum
algorithm--had it except needed to take 2s complement. Now communicating w/ MIOC. Another thought--would it
be better to actually make networks into programs within the MIOC--then I have some kind of verification of what
was actually programmed. Think about it).
So, now all that's left are experiment parts? No, the big one is multiple induction stimuli. We have node 0 as big brother
the one who listens to everything else, and the one who does the first induction sequence. But if we want additional
induction sequences...do the logic within node 0, or simply add nodes after the tapper nodes?

2/23/04
Was home yesterday, but worked substantially on this yesterday and today.
Including today, have implemented multiple experiment parts AND multiple stimulus induction sequences. This was a lot
of work to do right. Decided the right way was to do the logic within node 0 (big brother) rather than the hack of adding
additional nodes on the end. This was done most elegantly by subclassing RNTapperNode to make RNBBNode, with additional
instance variables to hold an array of flash-intensites and timers, as well as the number of subnodes. The flash
methods now take a subChannel as input (note, my terminology is a bit variable here--using stimulusChannel mostly, but 
subChannel occasionally). Also added methods to convert back and forth between MIDIChannels and stimulusChannels (this
conversion is fixed: stimulus 1 is channel 16, 2 is 15 and so on. For now the MIDI Note is the same for all induction
stimuli, but added a hook to modify this in the future. This required other changes--connections were elaborated to
have subChannel information, each node also needs to know not only if it's listening to BB, but which channel. All that's
left in this regard is to colorcode the different induction sequences and color nodes accordingly. What's best way
to have an application-wide array of colors? I'll just put it in the view--naah, nodes need to know.

MultiPart experiments--this was another biggie--now an experiment is defined by an array of 'experiment parts' which
now consist of either a stimulus or a network. This has a class RNExperimentPart. Still to do: scheduling the 
activation of each part. For stimuli, send their events to the MIDI system (scheduled ahead of time). For the network,
need instead to rig a timer to trigger the programming of the MIOC with the network. My only question here is where
to do this--it requires knowing not only the network, but also the MIOC. The experiment, network and parts do not know
about the MIOC, so seems best to do this in the controller. Was thinking that we could instead schedule it all
in the experiment, but use an invocation (provided by the controller) as the thing to trigger to program the MIOC. The
easy way would be to tell the experiment about the MIOC, but I'm against that. The info needed is the experiment start time
both NSDate and MIDITimestamp--the former is used for making timers, the latter for scheduling MIDI. In my
'startExperiment' method in the controller, I sync these up very closely. Still, it's cumbersome passing two things
around, so...sounds like a new class would be nice here, with two internal time representations (or, a category on NSDate?).
(But don't think categories can add instance variables, so new class is called for)
Finally we need to know the MIOC (and MIDI). Not nice to make experiment parts think about this, really.

Other developments: looked at preferences system--would like to store the midi interface chosen as a preference, but
that's low on the list. [3/14/05 done--took 15 minutes]

To do: schedule network programming (have done stimulus scheduling) [DONE]

3/9/05
Development has continued bit by bit in whatever time I can find--often a half hour before I go to bed, or while waiting
for long matlab jobs to finish. It's very enjoyable.
Since I last wrote, have implemented all scheduling of experiment parts--stimuli and networks are programmed at
the appropriate time in the experiment. Ultimately I did this using notifications--cleared up all the circular
referencing that I was going to have to add. The RNController is the main listener to the notifications and takes
care of updating the experiment and views.

Another big thing--I've developed a histogram view of asynchronies for each
tapper--modulo whatever stimulus they are listening to. This is nearly done--last thing was adding text for the IOI to
each one. Remaining: auto scaling of counts, what to do when stimulus IOI changes (will this ever occur? Wait until
I have a clear experiment planned before doing this), what to do with nodes that don't receive explicit stimulus--
right now they're synced to the first stimulus, but in case of subnetworks, they should be keyed to the stimulus of
whomever they receive the most inputs from. 

Re: drawing. Everything's antialiased, yielding strange appearances, sometimes. Also, linewidth is scaled to bounds rectangle
so when I had a long x and short y, vertical lines were nearly invisible, and text was compressed completely. So I
returned to bounds being in pixels, like the view. Just realized I should round the origin, so it's on a pixel boundary--
this might improve the ghosting of some lines. [DONE]

Another addition: added a list of the experiment parts: start time and description. Would be nice to hilight those
that are currently active. [DONE]
Would also like an experiment timer on screen--update with experiment time. [DONE]

3/10/05
Progress: have experiment timer now and have added alert sheets to warn if an action will result in lost data. This is
almost working. Some double free when stopping that I'll have to hunt down, plus clearing the network view doesn't
clear the histogram views--does dealloc do this, or do I need something more? 
[DONE--they were in fact gone, but
the network view wasn't clearing the entire frame before drawing, so they persisted on screen--that is even after
a subview is removed, what it last drew still remains--this could be changed, perhaps, in the dealloc method, but
drawing there seems a bit off.] 

What's left...maybe nothing, except to work on matlab to generate networks and also to read in XML data

3/14/05
Fixed up the bugs above, but there are problems if load a new network on top of an old one.
This is getting easier and easier to make new things happen: can now individually test experiment parts.
Also--add text next to each stimulus node. [DONE]

Also added defaults for the midi sources! Put this in the
lowest level: MIDIIO, which makes most sense. Thinking back to choi's solution--if the default is not found
use not connected, with that default name. Then, can plug in default device and have it connect automatically.
Rather than resetting the default--which should ONLY be done for a specific user choice (which may argue for it
going into the controller?) Much done in a short time. later: implemented this behavior. If default is not found, does not
connect to another device (thus changing default). Instead does not connect. Later on, if default device is reinserted,
(and we're still unconnected) it'll first test if it can change to this (if not, stays unconnected). This way, only
user choices do result in a change in defaults. (Note, to view application defaults:
 "defaults read iversen.john.RhythmNetwork")
 
 Ok, now: fix up reloading of network, work on matlab code to read data and generate network experiments.
 
 3/16/05
 finished a set of matlab routines to read AND write XML plists
 Testing this now: matlab.netdef -> matlabtest.experiment
 It works!
 
 To do: add some info to say when experiment stopped (e.g. if was ended early) [DONE]
 Also, notes aren't picked up when saving, and if empty don't show up in strucure after parse [DONE]
 Also, fixed re-loading of network--histogram views now disappear and there seem to be no memory issues.
 
3/17/05
 Noted as [DONE] all those accomplished.
 
 What remains: matlab to generate different classes of networks
 A stop stimulus button by the test part button to stop all ongoing stimuli (now have to wait until they finish
 by themselves). [DONE]
 
 Additions: probably not for now: allow creating/editing networks and stimuli from within the program. Simplest would be
 to type in string representations. More elegant would do it graphically. this is a large project, and not really
 necessary, though I'm sure it'd be fun. For now, drop it. Do it in matlab instead.

 3/24/05
 Working in matlab most recently: wrote mfile to generate an experiment structure, convert to xml. Tested this--reads
 correctly in RN.
 Mainly have been analyzing gallop/stream data, so this has been on the back burner. Thinking about an experiment I've been
 wanting to do: paradiddle max speed controlling which hand's sound can  hear (both/single). From expererience,
 if I focus on the pattern being made by one hand, I can go faster--the other subordinates to that hand. Thought of
 a way that RN program could actually run such an experiment: 3 nodes: two input for hands, third output. What you'd
 like are changes in the volume of the two inputs--which means weighted links. My RNConnection class suppports 
 a weight, but this has no partner within the MIOC level. The MIOC is capable of this, so question is whether to
 create another class MIOCVelocityProcessor, which hews to the design of the MIOC, or, instead, keep the notion of
 weighted connection to the fore and fold it into MIOCConnection, which would emit two MIOC processors: one for the connection
 and one for the velocity map. Right now MIOCModel is all in terms of setting and breaking connections, so I'm loathe
 to add lots of stuff for maps. Since this is a big change, I decided to start up SCM and invested a few hours in
 learning Subversion and getting that set up. In fact, the editing of this file is my first modification of this project
 and will be my first commit. What I wanted is simply a way to get a snapshot of the project before embarking on these
 changes, so I could always roll it back to the current state. Still unclear: do I do this with version number or
 by creating a new branch or tag? E.g. do I say: revision # 3 is the last one without weighted connections (and remember)
 or do I create some more symbolic tag? What if I make other changes yet only want to revert a subset of files to their
 earlier state (assuming no interactions...) This is what I want to do... well, could do either--a branch is more discrete,
 and self documenting, a 'snaapshot'. As far as mixed revisions, I suppose I can svn update individual files to arbitrary
 revision numbers. Must choose revision log messages informatively.
 
 a few more questions: in xcode how control options to diff when I choose that button in SCM panel of a file's info?
 by installing gnu diffutils. 
 
 Finally, took a stab at setting up viewCVS web interface to my repository--looks great! This took quite some fiddling
 to get working. I've also started using revision control on various config files that I'm editing-that'll allow
 easy merging, e.g. into a new system install.
 
 4/1/05
 On way to Ann Arbor and then CNS. Been working on CNS stream poster and a little subversion. Will make a branch now
 
 4/7/05 at bennington
 Did it. Now realized a major flaw in weighted connections--there's no way to do it, really. can only weight input (to all
 destinations) or output (from all)--that is, velocity processor is a node, not a connection, property. Could use port 7 as some kind
 of patch panel to work around this, which could get me up to 16 weighted connections, or only use it in simple situations. 
 As this is a refinement, at this point, perhaps better to abandon it all together. A much more elaborate change would
 be to use the computer as the matrix, allowing such weighting and DELAYS. Of course, this would mean a completely
 new program, but it wouldn't be too hard. What would latency be in this case?
 
 4/12/05 at JFK
 On my way home from CNS conference. 
 
 6/24/05 in air
 On way to Ann Arbor. Just browsing around. Will get this experiment started this summer. Not sure if there's much
 to do at the moment. I added MIOC velocity processor. No need to keep working on a general weighting scheme for now.
 Could consider adding vel processing to each person's input (if output from triggers isn't adequate in some way), or
 to actively censor/boost individuals. This would require an additional experiment part, probably--it's a per-tapper
 property that would be set, as opposed to a network property.
 
 Looking over it again. Pretty complete. Thinking about actual experiment--how compensate for people's different
 tappping strengths? 1) just clamp them all to a fixed velocity (if on output from trigger then we lose all velocity
 performance data, so we ought to do it on input processor for each node: options: pass as is, compress, clamp to
 a specific velocity value.\
 
 Thought up some new experiment designs (experiment ideas.txt) that could make use of a global 'connection strength' that
 would modulate velocity (at input or output?) at input, controls what you hear of me, at output, controls what i hear
 of others. Could be either, but perhaps on output, representing how well a node is coupled to the rest of the network.
 Another is capability for randomly timed inputs--to probe reaction/anticipation transition. Gradients of these kinds of
 properties, rather than step changes, would be ideal. General class: parameter gradient. Start value, end value, start
 time, duration, computed end time, curve (linear, accel/decel) a timer for changing the value, a target object that 
 the parameter value is plugged into each time it changes...
 will require more MIOC programming--how well will it handle this? More generally, how do realtime control of parameters?
 Man, this is starting to sound a lot like a general sequencer? Tempting to make it all in software, but unsure of capacity
 limits for that. Would probably need some kind of firewire/multiple port midi interface. Can't search on this now, 
 unfortunately. 
 For random stimulus: mean ISI & std deviation from mean, or continuous from min to max?
 
 6/26/05
 On way home from Ann Arbor. Dad's Birthday.
 Added a jitter parameter to stimulus (and RNBBNode). For each event, uniformly distributed jitter is added.
 
 6/27/05
 checking accuracy of recordedEvents in matlab--isochronous is right on, with errors in 10s of ns. This is the
	time recorded by a simple loopback. The jittered one didn't quite make sense--some IOIs were outside the
	jitter range...For full accountability, store in 'partTiming' record the requested times of stimulus events.
DONE: fixed jitter (was jittering the asynchrony, in fact, so that IOIs could be between IOI+/- 2*jitter,
	which includes zero. Now jitters the IOI, appropriately. Stimulus saves a list of requested event times
	relative to experiment start, which are saved as part of the partTiming record in output file. Field name
	is subEventTimes. Finally, had to change RNController to, after programming the part, store the stimulus's
	eventTimes into the part's subEventTimes. Changed RNExperiment to save the new part data. Will commit. rev 30
	
Next: velocity processors for input and output of nodes--this'll allow things like manipulation of coupling strength
	(but not generally weighted edges). Question is whether we need nodes to have individual parameters, or whether
	it's enough to have a single global processor applied to all nodes. Uses: input processor can help normalize
	performance velocities...but no, then we lose any performance details, so input processors won't be useful at all.
	That leaves output processors, which means a processor right before hitting the sound generator. Affects all
	inputs to a given node. (remember, nodes are addressed by channel for a single port)...That's economical,
	but if we had each performer's output to one port, then we really could have individually weighted links. That's
	not necessary now, only thinking about a global coupling strength (though, gosh it would be nice to have some
	weighted links...)
	
	For now: adding velocity processors on output side. Add a collection (array in this case) of velocity processors
	in addition to existing one of connections. These two now represent the state of the MIOC. Add a velocity processor
	to each node, too. Need a way to change these velocity processor values during the experiment--some kind of new
	experiment part, I think, defining a set value, or perhaps a gradient over some period of time. For the actua
	processors, have a simple normalizing one--fixed velocity, and one that modifies the slope (a weight), perhaps
	with a lower shelf. The first will hide any differences in individual tapper's tapping strength.
	
7/8/05 Now...what was I doing? Ah, yes. Refactoring to combine common code for processors. Defined a formal protocol
MIOCProcessor (in MIOCProcessorProtocol.h) and added some protocol requirements to my more general code. The main
point is that to qualify as a processor, an object needs to have a MIDIBytes method to express itself in a way
that the MIOC can understand.

I _could_ centralize all of the processor add/remove in MIOCModel, but that would be mere busywork--the semantics
of different processors are such that it makes more sense to have dedicated add/remove methods for the two--we  have a
list of connections in the netrowk, but no centralized list of velocity processors--each node has one. I did
consolidate the final method that creates MIDI message for a processor.

7/19/05 Wrap it up so I can begin testing with the MIOC. Decided to stop w/ development of velocity processing for now.
I have enough done to begin to test the hardware and figure out such practicalities.

7/20/05 Working w/ full setup today. Fixed connection lists so that computer BB hears everything. the MIOC is
working absolutely great. One thing is that if it's powercycled, all connections are fogotten (good). But might need
a 'reset' button to remove all connections in my model to sync the two.

Drum machine setup: 
Now picking out some sounds:
206 Lo Clave
215 Md Block
216 Lo Block
198 HiCgSlp
Drum machine setup: 
MIDI: drums 1-6 are notes 65-70; drum 12 is note 64 (pacing stimulus)
Channel: node#; Drum in ON V1 (linear velocity map); Drum out OFF (for now, testing it's on)
Clock in/out both OFF; MIDI thru OFF; Program change OFF (though could use this in future); 
Note map NORMAL (*in future, could use this to expand # of individual drum sounds using multi)

Drum set: set 0; pads 1-6 same sound (now Lo Clave), volume 99, panning center, tuning 0, asn multi, output main,
pad 12 same, but different sound (HiCgSlp).

This'll be basic set:  all other tappers have identical sounds. Could also make each tapper sound different using
panning, tuning and/or different sounds. Stay from panning, as that implies directional hearing (and consequent
grouping of others in different parts of space). But can use tuning and different sounds. For future, might have
each drumset consist of one or two sounds, spanning full tuning range.

Using Sysex Librarian, saved this as "SR-16 BasicSet.syx"--then set channel number differently for each node

Trigger->MIDI converter setup
MIDI Channel: 1-6 (node number, same as trigger #)
MIDI Prg chg: off
velocity curve: ld1
MIDI Note: 64+node number (65...)
threshold: 1
sensitivity: 11
trig type: adt
scan time: 1.0
retrig cancel: 1
mask time: 64
xtalk cancel: off

First tests
Realized event histograms were collecting all the time, not only when experiment running. Plus, they did not reset
after stopping/saving/restarting an experiment. Consequently, they overflowed. Increased the buffer size, but now
need to fix the actual recording. Otherwise, things look good. Did fix the drawing code when a new tap was added:
it was using a different coordinate system, leading to random bars appearing briefly before entire view was redrawn.
??Why is entire view redrawn every tap? How often is it redrawn--use Quartzdebug

Would like: some user interface to design networks, of course. In the mean time, would be nice to be able to adjust
how loud the pacing stimuli are. Easiest way: adjust in the drum machine.

ToDo: 
[done] Don't have histograms record continuously (but my soln also stops flashes--improve this)
Quitting: prevent quitting w/o saving first
when quitting, disconnect everything (else will be out of sync when restarting)
--how catch quit before it happens--app delegate?
add a 'reset' connections button to drawer--remove all connections & query to powercycle MIOC to be sure
[done] at end of experiment, unhilight expt parts
[done!] filter active sense at MIOC input, as well as redundant note offs--added new class for filters. Makes it clear
I could factor a processor base class out of all of this.

Bug: if midi is not connected, prog crashes on start as MIOC model attempts to send sysex during initialization.
Easy solution: make sure a destination exists before starting. This doesn't work if default connection isn't there...
So, doing the better solution: adding error checks to MIDIIO--if no destination exists, or there is some other error,
this is noted and propagated up layers until original call in MIOCModel--if there was an error, it can be handled.
For now, simply logs. However, ideal would be to 1) alert user that there's a problem connecting w/ MIOC, and
2) do something to re-synchronize state: If some things have been added, remove everything and ask user to powercycle
the MIOC to clear all.

*Added: can now query MIOC for its name. If we get it, post a notification allowing UI in MIOCSetupController to sync.
But, when to send query--we now do it on startup, but what if the MIOC isn't connected then? How poll for MIOC being
online? 
Otherwise, any sysex we send, we assume is received correctly. could wait for an acknowledgement before continuing--but
what do do if we want to send and are still waiting for an earlier acknowledgement? We have to wait, or block, which
isn't so cool. What's best way to deal w/ such asynchronous IO?

matlab: creation & analysis

7/22/05
First experiment today! Ani and myself. Today I spent time setting up the hardware for multiple tappers. All worked
very smoothly, and the program worked without a hitch! Now there are so many possibilities. The first one simply
showcased various connection possibilities & went: forcing, no feedback; forcing w/ feedback; continuation; join
(initially we had different tempi, then had to synchronize w/ eachother)

What would be cool is an online display of ITI for each tapper. Histogram view could be more general. However, a node
knows how to draw itself, so should it be the one storing the information and doing the calculations?

Also created a test setup that allows all people to check self monitoring, and lets each node in turn be hooked up
to BB, e.g. for sending out Sysex dumps to the individual drum machines. Right now I have a master machine that I
suck the sysex from using Sysex Librarian. Using the test netdef, I can send this out to each drum machine (could
simply broadcast it, too.) Then I need to manually change the MIDI channel for each machine (Right now, let's try to
reverse engineer the sysex format to find where that channel is stored...not trivial--there are a few differences, but
I don't see any pattern yet--i.e. it's not as simple as a single nybble showing the MIDI channel).

A wierd one--if you hide the main window, tons of exceptions get raised to do with flashing the node

Thinking about timing. When I schedule a new network, currently have 30ms preroll, and things trigger then. But that's
the start of programming, not the end, when all is done (in fact I have no good way to measure that). But...given
MIDI rate of 31500 bps and 10 bits/byte and fact that one connection message is 18 bytes, it'll take ~6ms to send
each connection/disconnection! So if we have 10 links to change, that's 60ms. So, the truly needed preroll depends on
the complexity of the difference of the network and previously active one. Could analyze that, of course, but that doesnt
change the fact that the updates will be spread over a range of time. Just have to decide when I want the update to finish--
right AT the moment or averaged around it. All things considered, the 30ms preroll will often satisfy the latter, so
we'll keep it as is.

So, what to do next. 1) MIOC reset (is there a command I can send, or do I have to powercycle?)
2) keeping of real time stats on tapping. Have hist, which is based on a list of event times (and asynchronies?).
Want something more general--how split it. Have the tapper node, a stats kind of object, a stats view...
Would also be cool to have another panel showing a running ITI plot for all the tappers, color coded, together with
target ITIs...Now, update regularly, or only on receiving a new event? How does info get to histograms now?
--who has MIDI listener method? 
RNExperiment		--	Note On only. Convert event time to experiment time. append to event list
RNNetworkView		--  Decipher node# from channel/note; flash node; add event to appropriate histogram view
--Sysex listener...
MIOCModel			--  Parse expected responses to queries
MIOCSetupController	--  Simply display it in the drawer

Now, reason we  have it going on in network view is that we need to tell the nodes which view to draw in...that is
the flash updates don't occur within the view object, but the node object. It makes more sense to have the node
worry about its own events, but we don't want to send to each one, so instead have the experiment (or network) do
the delivery. I'm starting to like the idea of a stats object that keeps track of a list of events & asynchronies,
and knows how to display same. Q--does this need to be separate from the node. Well, it needs its own view...and
I don't like the idea of a node being a view. I guess we could have a node and a node view. I seem to recall that
pattern. But who knows about who. Do subviews need to know about parent? They do get attached to the network view,
so that's ok. so generalize RNNodeHistogramView to RNNodeView? But then what does a node actually do?? A repository
for MIOC port info and...does it know about stimulus it's listening to? What it's connected to?
[will I ever need a node to tune into multiple stimuli??] 

7/25/05
Hi. Spent (wasted?) about four hours today on verifying MIOC is online and MIDI is connected correctly--now it will
test and complain if it can't find MIOC, give you a chance to turn it on/connect MIDI, and then try again. If MIOC
is simply off, this works fine. If MIDI is disconnected, something's not yet working--it can't reach the MIOC.
Why this was a waste is that for wednesday I'll be here and these issues won't come up--it's more for someone else
who is using the system and needs help. Ugh. Really I did it just becaue I was thinking about it and it was a challenge.
What I really want is some kind of online display of ITIs! And a reset ability. the second, I'll do by hand for now.
The former...more useful for the experiment! Another goal is to program the experiments carefully and get that ready! So
these four hours really weren't on target. Also done today--bought the rest of cables, etc needed and did some work on
marking up and securing devices to the sliding tray. Cable housekeeping. Main goal is for them to be clearly identified
and with little risk of being accidentally pulled out. Also spent time fixing a svn problem with interface builder, and
adding decoding of sysex responses to drawer.
[later] Fixed up the connection verification and reset logic. Problems w/ MIDI connection need user to interact w/ 
the MIOCSetup drawer, so have to exit the modal dialog and ask them to fix then press 'reset' button. Not great, but
works. Also had to give them a way out of this infinite loop, as well as when you don't really want to bother connecting
the MIOC. Still: simply bails if MIOC is offline and try to send other messages.

Expt ideas:
initially collect behavioral data on individuals
preferred tempo, deviation
ability to tap to driven tempi
ability to react to random stimuli

networks,
compare ring, nearest neighbor, all connected

Online data display
Need to think about the architecture
I have tappers who have individual events and possibly a stimulus associated with them
An experiment that contains the current state of things: stimuli and network, as well as the parts and timing, 
defintion and saving of events
A network with nodes and connections as well as some lower-level translations of this info to use in connecting to MIOC
and also decoding MIDI input into a node number
node knows its MIOC port and channel for I/O, and some other stuff useful for drawing them
Network view, knows its network, as well as histogram views for each node
the histogram views keep track of their data

Implemented RNDataView, and added this to the interface. Haven't thought about how to drive it, however. First show
ITI ongoing. X scale? scrolling or fit entire duration of experiment within the window. Do the latter first, and also
mark on it stimulus and network start/end points. How indicated different tappers? What I did: plots ITI using different
colors and expands the x range once graph reaches the limits.

7/27/05
Just did first full experiment and it was a success. Things worked without a hitch, basically. With a full set of 6 people,
the flashing of nodes on screen got very lagged behind, so I commented that out. Useful for testing, but in this
case the trigger-midi box shows each tap clearly. Perhaps keep showing the stimulus output as flashes. I was worried that
this might invalidate the data, but fortunately that's all timestamped on arrival, so it doesn't much matter how
long I take to process it after that. A few things that need improving--working on experiments is a pain--using the
text editor; hard to get an overview of things, and prone to error. Also the timing was a bit of a hassle. Would be easier
to use delta times, delta network defs: add, remove, etc. Also total experiment length--sometimes I wish it would go
on longer. Unfortunately, we stop recording data once timer stops--would be better to let that continue indefinitely
for those cases where interesting things are still happening. The program did crash onece after running a long time
because of the malloc stack logging I'm doing--turn that off!
ITI display was very useful--nice overview of experiment. Relative phase would be interesting, too, or asynchrony.
The little histograms and ITI readout were cute, but I didn't refer to them (of course, I was taking place in the
experiment, so I didn't watch. 
To do: better experiment programmability--change timing, etc on the fly?
program change to change sound sets
keep loging after experiment stops

Just found SR-16 sysex format document...including drumset definitions. You can alter the drumset on the fly
using 72 data bytes (~24 ms transmission time). Now--when we're transmitting out the interface, we're not blocked
from receiving with time precision, right? _this_ could be used to implement arbitrary weightings--by setting the
pad volumes on the fly.

However, to set overall coupling strength, best is to do it only once, for each tapper--filter on output side is best, as 
will still record normal velocity performance data...

Did I write about this before?: could have a single master node-flash timer that quickly loops thru all nodes and
adjusts color, rather than a new timer being created for each flash and each node. When all flashes extinguish, could
pause itself, and then re-establish when a new flash happens. This could be a class-wide object. Another optimization:
color is specified for each flash, released and retained. Instead, have a setter for color, and flash just works
using that color

8/28/05 Has it been a month already!? I was away, true, but time flies. for now, not to worry about the flash displays.
Also stared in with quartz debug to see how much is being redrawn each time. I took out a 'set redraw' kind of code
for each flash--that caused entire view to redraw. Instead, need a way to add to dirty region only the tiny bit I've
flashed...

For next experiment, focus on the connection strength global parameter--add a velocity processor to output to each
tapper. Add a new kind of experiment part that ramps the weight from start to end over a given duration...

9/6/05 Gearing up for next experiment.
As for drumset changes, on the fly would be cool, however for now could pre-program them in each machine (don't forget
to manually set MIDI channel for each machine) and then use program changes sent to all. Note, this'll require that
all outputs are listening to the computer--define a separate channel only for control messages (channel 1 from computer).

However, this is of lower priority now (can always ask each tapper to do the change manually).

Higher priority--rewrite all experiments, employing improvements from earlier. Better yet would be ability to
edit from within rhythm network program, as doing it as text files is a little difficult. That's another project for
another day (week?).

Also, try different sets of drum sounds--program in various banks into drum machines and have people swap them. This might
be best to pilot with only two people. It'd be nice to have a central facility for setting up drum banks within RN.
Again, for another day.

For RN programming: implement global connection strength. One way is via volume filters alone. Another will play
distracter noise tones.

Today's changes: Reorganized netdef and experiment files: organize by experiment. Change load code to point to
new location. (someday: make base directory a preference). DONE

Study best way to do velocity control. Looking at experiment parts--was programmed ad hoc, might want to have a 
category or protocol to which anything wanting to be a part must conform. Right now, there's a lot of special
case logic in RNExperimentPart. Postpone.

I think the case I want is one in which the degree of coupling is ramped up from zero to full. I'll specify the
start time and duration of the ramp. (more generally, also the beginning and ending weight). This'll need
its own timer to send network-wide weight changes. We'll have to loop thru nodes and transmit a processor for each one.
Review velocityProc. Problem: velocity processor slopes are quantized, with step of 0.125, so there are only
eight possible values between 0 and 1. Possible solutions: 1) concatenate two processors, 2) manipulate offset, not
slope, 3) set to specific constant value (ignore source's velocity).

Have gone and looked at velocity data for original experiment, and find large inter-subject differences. Some
are heavy tappers, others are light. For some experiments, this is desirable, in a way, allowing people to express
their urgency. However, it means that different people may have different perceptual weight--this is a question:
do people w/ louder taps act as leaders? Do they sway others more readily? In the coupling experiment, however,
I think I want to eliminate this variation.
To do: add a 'constant' velocity processor (Done).

Now, for the part, we have two options: one is to ramp the slope, other to ramp the constant velocity. We'll add a
'type' field: 'weight' or 'constant', with parameter ranges [0,1] and [0, 127] respectively.

9/7/05 great progress: have the definition and input side done: specify a globalConnectionStrength or 
globalConnectionStrengthRamp experiment part. The latter is expanded into multiple globalConnectionStrength events.
This involved a new class RNGlobalConnectionStrength and changes to RNExperimentPart and RNExperiment (the methods
that do loading of an experiment and creation of experiment parts array), as well as
MIOCVelocityProcessor (setters for constant map, and to change port and channel, which we'll need to do when associating
with nodes).

Today: implement scheduling of this new part type, a timer handler (in RNExperimentPart). Then implement a notification
handler in RNController that will assign the new velocity processor to all nodes in the network (who owns nodes? RNNetwork?)
this also must compose and send control messages to the MIOC--use add/remove velocityProcessor type mechanism.

Ok, this is for the most part done, along with a raft of other changes that were needed to graft on this addition: to
MIOCVelocityProcessor, MIOCModel, RNTapperNode...
But...just realized a problem--we don't necessarily want peoples OWN feedback to also change velocity, but
unfortunately, that will also happen...it's a GLOBAL weight. How can I treat people's feedback differently? Rightnow
it's on output that velocities are filtered, which means all sounds played out are modified. We're using a single
channel to address each device, so can't do a channel-based solution...I don't see any way out here. We'll have to
try it and see if it works. Thought drum machine might be able to change velocity map for each voice, but no. That
would have worked. Any way I can do this w/ filters--e.g. snatch one note and do something with it. An alternative would
be to have each output be sent to a unique port--this'll make future upgrades difficult. Another more complex possibility
would route self taps directly with no velocity change (or a high constant one) and route taps desitined for other
tappers to a special port (one channel for each stream to be modified: for now one per tapper, with same channel number.
This'll allow us, say 6x5 = 30 tappers, use two ports for this filtering, giving 32 possibilities.
This is opposed to 6*7 = 40 tappers. What a pain! so: port1,chan1(I) -> port7,chan1(O) FILTER port7,chan1(I) ->port1,chan1(O)
I'll also look into splitting: three zones: below our note, our note, above our note

9/8/05 Finished up implementation of connection strength ramp--I like the constant strength one better than the slope,
allows for smoother change. It does remove individual velocity info. Hmm. We could do a slope from 0 to 2, say (main
reason I didn't like was that typical taps are nowhere near max velocity, so slope scaling doesn't give much change...

On trigger-midi, reduced threshold to 0 (min) and boosted sensitivity to 14.

Today, implement the second path. This means--a second pair of routings for each node's connections
	split off input from a given channel to same channel out (if self feedback)
	and to same channel on port 7, which is looped back to port 7 input, which is then routed to the OTHER connections
	that tapper goes to
	so e.g.
	{1p1, 1p1} as usual
	{1p1, 2p1} --> {1p1,1p7} & {1p7,2p1}
	
	This is taken care of simply when instantiating the net--add code to do this. Also need some kind of global
	switch to say this is what we want (so we don't need it for non-conneciton strength experiments)--this'll be
	in a separate field, call it 'isWeighted' BOOL YES?NO. Also, initialize velprocs to pass through
	put velprocs on output to self, AND output to others (port 7)
	Then, expand on node to allow setting a connection strength for self feedback, and to others
	
Tapper (vp) IN port 1----(direct feedback) ----- (velproc) OUT same channel, port 1
	            \																			 /------ OUT chanA, port 1
	             \																			/
		          \---(vp) OUT same channel, port 7 --(wire link)-- IN same channel, port 7 ---------- OUT chanB, port 1
																							\
																							 \------ OUT chanC, port 1
																							 
9/9/05 Great, just tested this out with two inputs and it works! Self input goes unchanged, while other's input
is modified, as desired. What I need now is a way to also modify the self feedback--since it's only about 50%
of the velocity for a typical strength tap, there's not as much dynamic range. Simplest would be to hang a
velocity processor on each input to either boost the slope or set to a constant value. Also: check how accurately our
velocity ramp changes are placed in time--might want to siphon off some input to the computer from port 7 to know
for sure what velocity each tapper is hearing, but need to worry about affecting timing of tapping data--if there's
a lot of traffic, how inaccurate does it become? Looked at it, and it's +/- 20ms for the sending of sysex for
each velocity change. Could I schedule these as midi packets and use core midi to do the scheduling for me? Look into
this. Now, the output it needs to compete with are the pacing stimuli--if we're in midst of a long sysex burst, it
could delay a pacing stimulus, no? How does the server prioritize things? Will it interrupt a sysex to play a
time-requested event? What does it do if many things are scheduled to go out the same time? Again, how does it prioritize?

Also, need a way to set filters to: no filter/ remove them: can easily do this using events in the experimentParts
--put a weight=1 at start. Also, think of adding an event to modify the input velocity mappings: both tappers and
computer volume. For the connection-strength, want input at max, so we'll set it to 127.

Also fix up flashing code--simply turn it off for now--plenty of feedback on triggerMIDI converter

Finally, a way to setup drumsets--via program change messages
